{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testando SONAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IP0PolkOVqdE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"../data/train_preprocessed.csv\")\n",
        "\n",
        "text = df['comment_text'].values[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UqoR9dgpWvdu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline\n",
        "\n",
        "embedder = TextToEmbeddingModelPipeline(\n",
        "  encoder=\"text_sonar_basic_encoder\",\n",
        "  tokenizer=\"text_sonar_basic_encoder\",\n",
        "  device=torch.device(\"cuda\"),\n",
        "  dtype=torch.float16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbSs2NT0WgPH",
        "outputId": "0135bc88-6121-413d-fce8-32835e5cbec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([100, 1024])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sonar/inference_pipelines/text.py:259: UserWarning: For 3 input tensors for SONAR text encoder, the length was truncated to 514 elements.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "with torch.inference_mode():\n",
        "  embeddings = embedder.predict(text, source_lang=\"eng_Latn\")\n",
        "\n",
        "print(embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ4waYaBbbR2",
        "outputId": "2b42bd9b-9d84-4fa8-be4d-d7e36a089819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto original: explanation why the edits made under my username hardcore metallica fan were reverted  they weren t vandalisms  just closure on some gas after i voted at new york dolls fac  and please don t remove the template from the talk page since i m retired now \n",
            "Texto reconstruído: Explanations why the editing done under my username hardcore metallica fan was reversed they were vandalism just blocking on some gas after i voted at new york dolls fan and please do not remove the template from the discussion page since i am now retired\n"
          ]
        }
      ],
      "source": [
        "from sonar.inference_pipelines.text import EmbeddingToTextModelPipeline\n",
        "\n",
        "decoder = EmbeddingToTextModelPipeline(\n",
        "    decoder=\"text_sonar_basic_decoder\",\n",
        "    tokenizer=\"text_sonar_basic_encoder\",\n",
        "    device=torch.device(\"cuda:0\"),\n",
        "    dtype=torch.float16,\n",
        ")\n",
        "\n",
        "with torch.inference_mode():\n",
        "  text_decoded = decoder.predict(embeddings[:2,:], target_lang=\"eng_Latn\")\n",
        "\n",
        "print(\"Texto original:\", text[0])\n",
        "print(\"Texto reconstruído:\", text_decoded[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "nl7Qf0zgdvRU"
      },
      "outputs": [],
      "source": [
        "sample_text = ['We love guns and freedom']\n",
        "sample_prejudice = ['racism']\n",
        "sample_virtue = ['love']\n",
        "\n",
        "embeddings_text = embedder.predict(sample_text, source_lang=\"eng_Latn\")\n",
        "embeddings_prejudice = embedder.predict(sample_prejudice, source_lang=\"eng_Latn\")\n",
        "embeddings_virtue = embedder.predict(sample_virtue, source_lang=\"eng_Latn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "HqF_yq8TeeAn"
      },
      "outputs": [],
      "source": [
        "embeddings_detox = embeddings_text - embeddings_prejudice + embeddings_virtue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "yEVFn6JrexZE"
      },
      "outputs": [],
      "source": [
        "decoded_text = decoder.predict(embeddings_detox, target_lang=\"eng_Latn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8sMUh4Ve6HI",
        "outputId": "926f5018-4466-4eca-d1fe-f34495501472"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'We love love and freedom'\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "pprint(decoded_text[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Teste de integração de uma desintoxificação simples (sem peso para cada categoria)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assumindo que:\n",
        "# - `df` é o DataFrame com os comentários\n",
        "# - `embeddings` é um tensor do PyTorch (shape: [N, 1024])\n",
        "# - `text` são os comentários usados para gerar os embeddings\n",
        "# Obs: os comentários devem estar na mesma ordem de `df`\n",
        "\n",
        "# Adiciona os embeddings ao dataframe\n",
        "df_embed = df.iloc[:len(embeddings)].copy()\n",
        "df_embed['embedding'] = [emb.detach().cpu().numpy() for emb in embeddings]\n",
        "\n",
        "# Define categorias para calcular\n",
        "categorias = ['identity_hate', 'insult', 'obscene', 'severe_toxic', 'threat', 'toxic']\n",
        "vetores_categoria = {}\n",
        "\n",
        "# Calcula vetores médios\n",
        "for cat in categorias:\n",
        "    subset = df_embed[df_embed[cat] == 1]\n",
        "    if not subset.empty:\n",
        "        vetor_medio = np.mean(np.stack(subset['embedding'].values), axis=0)\n",
        "        vetores_categoria[cat] = vetor_medio\n",
        "        print(f\"{cat}: vetor médio calculado com {len(subset)} exemplos.\")\n",
        "    else:\n",
        "        print(f\"{cat}: nenhum exemplo encontrado.\")\n",
        "\n",
        "# Também podemos calcular o vetor médio de comentários 'neutros'\n",
        "neutros = df_embed[(df_embed[categorias].sum(axis=1) == 0)]\n",
        "vetor_neutro = np.mean(np.stack(neutros['embedding'].values), axis=0)\n",
        "vetores_categoria['neutro'] = vetor_neutro\n",
        "print(f\"neutro: vetor médio calculado com {len(neutros)} exemplos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selecione um exemplo tóxico para testar (altere o índice se quiser outro)\n",
        "idx = 5  # exemplo: 5º comentário da base\n",
        "\n",
        "# Recupera dados\n",
        "texto_original = df_embed.iloc[idx]['comment_text']\n",
        "embedding_original = df_embed.iloc[idx]['embedding']\n",
        "categorias_ativas = [cat for cat in categorias if df_embed.iloc[idx][cat] == 1]\n",
        "\n",
        "print(\"Texto original:\", texto_original)\n",
        "print(\"Categorias tóxicas:\", categorias_ativas)\n",
        "\n",
        "# Aplica desintoxicação sequencial (simples soma dos vetores corretivos)\n",
        "embedding_editado = embedding_original.copy()\n",
        "\n",
        "for cat in categorias_ativas:\n",
        "    vetor_toxico = vetores_categoria[cat]\n",
        "    embedding_editado = embedding_editado - vetor_toxico + vetores_categoria['neutro']\n",
        "\n",
        "# Converte para tensor\n",
        "embedding_tensor = torch.tensor(embedding_editado).unsqueeze(0).to(\"cuda\").to(torch.float16)\n",
        "\n",
        "# Reconstrói texto\n",
        "with torch.inference_mode():\n",
        "    texto_desintoxicado = decoder.predict(embedding_tensor, source_lang=\"eng_Latn\")\n",
        "\n",
        "print(\"Texto original:\", texto_original)\n",
        "print(\"Texto desintoxicado:\", texto_desintoxicado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Teste de integração da desintoxificação com peso para cada categoria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = np.stack(df_embed['embedding'].values)\n",
        "Y = df_embed[categorias].astype(int)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Modelo base binário\n",
        "base_model = LogisticRegression(max_iter=1000)\n",
        "multi_model = MultiOutputClassifier(base_model)\n",
        "\n",
        "# Treina\n",
        "multi_model.fit(X_train, Y_train)\n",
        "\n",
        "# Previsão com probabilidades\n",
        "probs = multi_model.predict_proba([X_test[0]])\n",
        "pesos = [p[1] for p in probs]  # pega a probabilidade da classe 1 para cada categoria\n",
        "print(\"Probabilidades para cada categoria:\", pesos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "E = X_test[0]\n",
        "E_editado = E.copy()\n",
        "\n",
        "for i, cat in enumerate(categorias):\n",
        "    E_editado -= pesos[i] * vetores_categoria[cat]\n",
        "\n",
        "E_editado += sum(pesos) * vetores_categoria['neutro']"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
