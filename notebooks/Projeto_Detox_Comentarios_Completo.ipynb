{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b957ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install pandas numpy scikit-learn matplotlib transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv(\"train_embedded.csv\")\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(ast.literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7343c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_center(df, mask):\n",
    "    return np.mean(df[mask][\"embedding\"].to_list(), axis=0)\n",
    "\n",
    "v_neutro = get_center(df, \n",
    "    (df[\"identity_hate\"] == 0) &\n",
    "    (df[\"insult\"] == 0) &\n",
    "    (df[\"obscene\"] == 0) &\n",
    "    (df[\"threat\"] == 0) &\n",
    "    (df[\"toxic\"] == 0)\n",
    ")\n",
    "\n",
    "v_racismo = get_center(df, df[\"identity_hate\"] == 1)\n",
    "v_insulto = get_center(df, df[\"insult\"] == 1)\n",
    "v_obsceno = get_center(df, df[\"obscene\"] == 1)\n",
    "v_ameaca = get_center(df, df[\"threat\"] == 1)\n",
    "v_toxico = get_center(df, df[\"toxic\"] == 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v_corrigir_racismo = v_neutro - v_racismo\n",
    "v_corrigir_insulto = v_neutro - v_insulto\n",
    "v_corrigir_obsceno = v_neutro - v_obsceno\n",
    "v_corrigir_ameaca = v_neutro - v_ameaca\n",
    "v_corrigir_toxico = v_neutro - v_toxico\n",
    "\n",
    "vetores_corrigir = [\n",
    "    v_corrigir_racismo,\n",
    "    v_corrigir_insulto,\n",
    "    v_corrigir_obsceno,\n",
    "    v_corrigir_ameaca,\n",
    "    v_corrigir_toxico\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.stack(df[\"embedding\"].values)\n",
    "Y = df[[\"identity_hate\", \"insult\", \"obscene\", \"threat\", \"toxic\"]].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detox_embedding(embedding, probs, vetores_corrigir):\n",
    "    if probs.sum() == 0:\n",
    "        return embedding.copy()\n",
    "    weights = probs / probs.sum()\n",
    "    v_corrigir = sum(w * v for w, v in zip(weights, vetores_corrigir))\n",
    "    return embedding + v_corrigir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e0302",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"facebook/sonar-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def sonar_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c06384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comentario = \"You don't belong here. Go back to your country.\"\n",
    "embedding_original = sonar_embedding(comentario)\n",
    "\n",
    "probs_raw = clf.predict_proba([embedding_original])\n",
    "probs = np.array([p[0][1] for p in probs_raw])\n",
    "\n",
    "embedding_corrigido = detox_embedding(embedding_original, probs, vetores_corrigir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad18d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_distance(a, b):\n",
    "    return 1 - np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "categorias = [\"identity_hate\", \"insult\", \"obscene\", \"threat\", \"toxic\"]\n",
    "nomes = [\"Racismo\", \"Insulto\", \"Obsceno\", \"Ameaca\", \"Toxico\"]\n",
    "vetores_toxicos = [v_racismo, v_insulto, v_obsceno, v_ameaca, v_toxico]\n",
    "\n",
    "for nome, vetor in zip(nomes, vetores_toxicos):\n",
    "    d_orig = cosine_distance(embedding_original, vetor)\n",
    "    d_detox = cosine_distance(embedding_corrigido, vetor)\n",
    "    print(f\"{nome.upper()} - Original → {d_orig:.4f} | Detox → {d_detox:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8369dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = [embedding_original, embedding_corrigido, v_neutro,\n",
    "     v_racismo, v_insulto, v_obsceno, v_ameaca, v_toxico]\n",
    "\n",
    "labels = [\"Original\", \"Detox\", \"Neutro\", \n",
    "          \"Racismo\", \"Insulto\", \"Obsceno\", \"Ameaca\", \"Toxico\"]\n",
    "\n",
    "colors = [\"blue\", \"green\", \"black\", \"red\", \"orange\", \"purple\", \"brown\", \"crimson\"]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, point in enumerate(X_pca):\n",
    "    plt.scatter(point[0], point[1], c=colors[i])\n",
    "    plt.text(point[0]+0.01, point[1]+0.01, labels[i], fontsize=9)\n",
    "plt.title(\"Embeddings em relação às categorias tóxicas\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
