{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f91be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27320fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Carregar dataset\n",
    "df = pd.read_csv(\"train_embedded.csv\")\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(ast.literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71402331",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_center(df, mask):\n",
    "    return np.mean(df[mask][\"embedding\"].to_list(), axis=0)\n",
    "\n",
    "# Vetor neutro\n",
    "v_neutro = get_center(df, \n",
    "    (df[\"identity_hate\"] == 0) &\n",
    "    (df[\"insult\"] == 0) &\n",
    "    (df[\"obscene\"] == 0) &\n",
    "    (df[\"threat\"] == 0) &\n",
    "    (df[\"toxic\"] == 0)\n",
    ")\n",
    "\n",
    "# Vetores tóxicos\n",
    "v_racismo = get_center(df, df[\"identity_hate\"] == 1)\n",
    "v_insulto = get_center(df, df[\"insult\"] == 1)\n",
    "v_obsceno = get_center(df, df[\"obscene\"] == 1)\n",
    "v_ameaca = get_center(df, df[\"threat\"] == 1)\n",
    "v_toxico = get_center(df, df[\"toxic\"] == 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83acc89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v_corrigir_racismo = v_neutro - v_racismo\n",
    "v_corrigir_insulto = v_neutro - v_insulto\n",
    "v_corrigir_obsceno = v_neutro - v_obsceno\n",
    "v_corrigir_ameaca = v_neutro - v_ameaca\n",
    "v_corrigir_toxico = v_neutro - v_toxico\n",
    "\n",
    "vetores_corrigir = [\n",
    "    v_corrigir_racismo,\n",
    "    v_corrigir_insulto,\n",
    "    v_corrigir_obsceno,\n",
    "    v_corrigir_ameaca,\n",
    "    v_corrigir_toxico\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af88121",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.stack(df[\"embedding\"].values)\n",
    "Y = df[[\"identity_hate\", \"insult\", \"obscene\", \"threat\", \"toxic\"]].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b4f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detox_embedding(embedding, probs, vetores_corrigir):\n",
    "    if probs.sum() == 0:\n",
    "        return embedding.copy()\n",
    "    weights = probs / probs.sum()\n",
    "    v_corrigir = sum(w * v for w, v in zip(weights, vetores_corrigir))\n",
    "    return embedding + v_corrigir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b97062",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "row = df.iloc[0]\n",
    "embedding_original = np.array(row[\"embedding\"])\n",
    "\n",
    "# Prever as probabilidades para o comentário\n",
    "probs_raw = clf.predict_proba([embedding_original])\n",
    "probs = np.array([p[0][1] for p in probs_raw])  # pega prob. da classe 1\n",
    "\n",
    "# Aplica detox\n",
    "embedding_corrigido = detox_embedding(embedding_original, probs, vetores_corrigir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f2d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_distance(a, b):\n",
    "    return 1 - np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "print(\"↔️ original → tóxico:\", cosine_distance(embedding_original, v_toxico))\n",
    "print(\"↔️ detox    → tóxico:\", cosine_distance(embedding_corrigido, v_toxico))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform([embedding_original, embedding_corrigido, v_toxico, v_neutro])\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=[\"blue\", \"green\", \"red\", \"black\"])\n",
    "plt.legend([\"Original\", \"Detox\", \"Tóxico\", \"Neutro\"])\n",
    "plt.title(\"Visualização dos embeddings\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54b0e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instalar e carregar SONAR\n",
    "!pip install -q transformers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_name = \"facebook/sonar-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def sonar_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c1cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inserir comentário manual e gerar detox\n",
    "comentario = \"Seu lugar não é aqui. Volta pra sua terra.\"  # Substitua pelo que quiser\n",
    "embedding_comentario = sonar_embedding(comentario)\n",
    "\n",
    "probs_raw = clf.predict_proba([embedding_comentario])\n",
    "probs = np.array([p[0][1] for p in probs_raw])\n",
    "\n",
    "embedding_detox = detox_embedding(embedding_comentario, probs, vetores_corrigir)\n",
    "\n",
    "print(\"Distância original → tóxico:\", cosine_distance(embedding_comentario, v_toxico))\n",
    "print(\"Distância detox    → tóxico:\", cosine_distance(embedding_detox, v_toxico))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
